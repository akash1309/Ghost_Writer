{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_Bot.ipynb",
      "provenance": [],
      "mount_file_id": "1GF1FaIyZQuBemmoH305hq2daEGbFHmCI",
      "authorship_tag": "ABX9TyPFODWPXo8TGDk4Q8VMBIuS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ztUsOZtxC4",
        "colab_type": "text"
      },
      "source": [
        "# Question Answer ChatBot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUsrjrM5t78j",
        "colab_type": "text"
      },
      "source": [
        "## 1) Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU1QksIAts3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1190f0e7-3e11-4e7f-cbd4-69d8fbac61be"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnp_rB1duGwb",
        "colab_type": "text"
      },
      "source": [
        "## 2) Reading data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYlwo1eruFkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For training\n",
        "\n",
        "with open('drive/My Drive/Pytorch_DataSet/Ghost_Writer/train_qa_one.txt','rb') as f:\n",
        "  train_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk7uqXwzub5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For testing\n",
        "\n",
        "with open('/content/drive/My Drive/Pytorch_DataSet/Ghost_Writer/test_qa_one.txt','rb') as f:\n",
        "  test_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWH8ujECukc-",
        "colab_type": "code",
        "outputId": "da3386d8-9da0-45fc-e751-353bd992ec3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112, 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNL7DzIVq3qk",
        "colab_type": "code",
        "outputId": "72b7bee7-63f8-43d4-8e9a-2a908594ba04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-TNws4TunrS",
        "colab_type": "code",
        "outputId": "41271693-9119-4663-d43a-429fd902b471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['the',\n",
              "  'coming',\n",
              "  'of',\n",
              "  'the',\n",
              "  'martians',\n",
              "  'no',\n",
              "  'one',\n",
              "  'would',\n",
              "  'have',\n",
              "  'believed',\n",
              "  'in',\n",
              "  'the',\n",
              "  'last',\n",
              "  'years',\n",
              "  'of',\n",
              "  'the',\n",
              "  'nineteenth',\n",
              "  'century',\n",
              "  'that',\n",
              "  'this',\n",
              "  'world',\n",
              "  'was',\n",
              "  'being',\n",
              "  'watched',\n",
              "  'by',\n",
              "  'martians',\n",
              "  'and',\n",
              "  'yet',\n",
              "  'as',\n",
              "  'mortal',\n",
              "  'as',\n",
              "  'his',\n",
              "  'own',\n",
              "  ';',\n",
              "  'that',\n",
              "  'as',\n",
              "  'men',\n",
              "  'busied',\n",
              "  'themselves',\n",
              "  'about',\n",
              "  'their',\n",
              "  'various',\n",
              "  'concerns',\n",
              "  'they',\n",
              "  'were',\n",
              "  'scrutinised',\n",
              "  'and',\n",
              "  'studied',\n",
              "  ',',\n",
              "  'perhaps',\n",
              "  'almost',\n",
              "  'as',\n",
              "  'narrowly',\n",
              "  'as',\n",
              "  'a',\n",
              "  'man',\n",
              "  'with',\n",
              "  'a',\n",
              "  'microscope',\n",
              "  'might',\n",
              "  'scrutinise',\n",
              "  'the',\n",
              "  'transient',\n",
              "  'creatures',\n",
              "  'that',\n",
              "  'swarm',\n",
              "  'and',\n",
              "  'multiply',\n",
              "  'in',\n",
              "  'a',\n",
              "  'drop',\n",
              "  'of',\n",
              "  'water',\n",
              "  '.'],\n",
              " ['was', 'world', 'being', 'watched', 'by', 'martians', '?'],\n",
              " 'yes')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj44BytWtuCh",
        "colab_type": "code",
        "outputId": "38c5884a-6f1d-44db-f8da-f772f2bd1cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "' '.join(train_data[1][0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'and we men , the creatures who inhabit this earth , must be to them at least as alien and humans are as lowly to martians as are the monkeys to humans . the intellectual side of man already admits that life is an incessant struggle for existence , and it would seem that this too is the belief of the minds upon mars . that , generation after generation , creeps upon them .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtQvroip1goT",
        "colab_type": "code",
        "outputId": "3f9dc244-6728-4fbd-b16a-cf3edf033a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yARv8Fnvu3e4",
        "colab_type": "code",
        "outputId": "b67bbd4c-c139-4f65-9b1e-01f1fe826a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "' '.join(train_data[0][0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the coming of the martians no one would have believed in the last years of the nineteenth century that this world was being watched by martians and yet as mortal as his own ; that as men busied themselves about their various concerns they were scrutinised and studied , perhaps almost as narrowly as a man with a microscope might scrutinise the transient creatures that swarm and multiply in a drop of water .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9eL3Iq4TVac",
        "colab_type": "code",
        "outputId": "70fb4f14-8f22-470c-9190-025c2104e460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "print(train_data[1][0])\n",
        "a = iter(train_data)\n",
        "while next(a):\n",
        "  b = next(a)\n",
        "  print((b[0]))\n",
        "\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(train_data[1][0])\\na = iter(train_data)\\nwhile next(a):\\n  b = next(a)\\n  print((b[0]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1tVMHKKvDaB",
        "colab_type": "code",
        "outputId": "67e90b73-bdac-4c7b-e80c-1e09f2b525d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "' '.join(train_data[0][1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'was world being watched by martians ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1WkErkpvQq7",
        "colab_type": "code",
        "outputId": "88469afd-d897-4ef2-f593-09b4ab53b629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "''.join(train_data[0][2])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmnTrFlgvT_c",
        "colab_type": "code",
        "outputId": "d97100de-278a-4d18-be62-4b3c8bb7daf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data), type(test_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(list, list)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRLLcWE6wK0Y",
        "colab_type": "text"
      },
      "source": [
        "## 3) Creating a dictionary.\n",
        "\n",
        "Creating a dictionary that contains all the words our train and test set has got so that the test data do not contain any word which is not present in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fNwT-RzvgKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = train_data + test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dolfQs4wwjx6",
        "colab_type": "code",
        "outputId": "86f89db4-509d-4270-8c32-2d0ba8f9547e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBF5UTBxi47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "\n",
        "for statement,query, answer in all_data:\n",
        "  vocab = vocab.union(set(statement))\n",
        "  vocab = vocab.union(set(query))\n",
        "\n",
        "vocab.add('no')\n",
        "vocab.add('yes')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wucVP9iQyBi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f8dLk9VyC2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the pad sequences in case if the string is too short or too long\n",
        "vocab_len = len(vocab) + 1 # 1 for padding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vUGGGYhyg-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, checking the length of the longest story that can be used in padding\n",
        "\n",
        "#Longest story\n",
        "\n",
        "all_story_len = [len(data[0]) for data in all_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZItciCw4zOcA",
        "colab_type": "code",
        "outputId": "5ed71295-711b-40c9-c423-344b18e87796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_story_len[:10]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[74, 75, 63, 64, 66, 50, 66, 60, 83, 60]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIEkIiOXzPgV",
        "colab_type": "code",
        "outputId": "905e5d6d-0778-4ddb-9753-1f9cf195bf5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_story_len = max(all_story_len)\n",
        "max_story_len"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPXKbWjfzYQK",
        "colab_type": "code",
        "outputId": "06cd0c25-ad2f-48fd-afb8-f41cd04b69b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_question_len = [len(data[1]) for data in all_data]\n",
        "all_question_len[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 12, 7, 6, 11, 10, 7, 7, 9, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdHIVIGYFeDU",
        "colab_type": "code",
        "outputId": "46b4a204-d1cb-4bb9-fe8d-5ba313977ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_question_len = max(all_question_len)\n",
        "max_question_len"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGKhXnKfitXn",
        "colab_type": "text"
      },
      "source": [
        "## 4) Vectorizing the Data\n",
        "\n",
        "Conversion of text into numerical values\n",
        "\n",
        "https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgDUNkVyFqAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_il-qPHfjD3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(filters=[])\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBDB72LdjoQM",
        "colab_type": "code",
        "outputId": "3b0c7bfb-523a-437c-b40f-4ef91c0aa58a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dodo': 1,\n",
              " 'ruddy': 2,\n",
              " 'laying': 3,\n",
              " 'infinitely': 4,\n",
              " 'previous': 5,\n",
              " 'through': 6,\n",
              " 'idea': 7,\n",
              " 'which': 8,\n",
              " 'stones': 9,\n",
              " 'bright': 10,\n",
              " 'soon': 11,\n",
              " 'do': 12,\n",
              " 'memory': 13,\n",
              " 'engines': 14,\n",
              " 'invaders': 15,\n",
              " 'commoms': 16,\n",
              " 'flying': 17,\n",
              " 'became': 18,\n",
              " 'six': 19,\n",
              " 'stooping': 20,\n",
              " 'punch': 21,\n",
              " 'peck': 22,\n",
              " 'bed': 23,\n",
              " 'nothing': 24,\n",
              " 'standing': 25,\n",
              " 'lived': 26,\n",
              " 'poor': 27,\n",
              " 'sparked': 28,\n",
              " 'lying': 29,\n",
              " 'shift': 30,\n",
              " 'or': 31,\n",
              " 'level': 32,\n",
              " 'blinded': 33,\n",
              " 'young': 34,\n",
              " 'sprang': 35,\n",
              " 'her': 36,\n",
              " 'rigid': 37,\n",
              " 'many': 38,\n",
              " 'utter': 39,\n",
              " 'kindly': 40,\n",
              " 'bridge': 41,\n",
              " 'fired': 42,\n",
              " 'these': 43,\n",
              " 'commom': 44,\n",
              " 'palings': 45,\n",
              " 'strange': 46,\n",
              " 'sentences': 47,\n",
              " 'jostling': 48,\n",
              " 'martians': 49,\n",
              " 'up': 50,\n",
              " 'unlocking': 51,\n",
              " 'occupied': 52,\n",
              " 'inkerman': 53,\n",
              " 'used': 54,\n",
              " 'foolish': 55,\n",
              " 'walked': 56,\n",
              " 'except': 57,\n",
              " 'intact': 58,\n",
              " 'fifteenth': 59,\n",
              " 'learning': 60,\n",
              " 'went': 61,\n",
              " 'recognition': 62,\n",
              " 'kingdoms': 63,\n",
              " 'things': 64,\n",
              " 'its': 65,\n",
              " 'the': 66,\n",
              " 'flattened': 67,\n",
              " 'quiet': 68,\n",
              " 'kitchens': 69,\n",
              " 'least': 70,\n",
              " 'given': 71,\n",
              " 'early': 72,\n",
              " 'sennacherib': 73,\n",
              " 'message': 74,\n",
              " 'advance': 75,\n",
              " 'putrefactive': 76,\n",
              " 'floor': 77,\n",
              " 'grew': 78,\n",
              " 'broke': 79,\n",
              " 'making': 80,\n",
              " 'second': 81,\n",
              " 'older': 82,\n",
              " 'prepared': 83,\n",
              " 'ex': 84,\n",
              " 'funnels': 85,\n",
              " 'parted': 86,\n",
              " 'writer': 87,\n",
              " 'altered': 88,\n",
              " 'disturbances': 89,\n",
              " 'nineteenth': 90,\n",
              " 'thudding': 91,\n",
              " 'telegram': 92,\n",
              " 'sides': 93,\n",
              " 'mean': 94,\n",
              " 'approached': 95,\n",
              " 'take': 96,\n",
              " 'lay': 97,\n",
              " 'growing': 98,\n",
              " 'compared': 99,\n",
              " 'motion': 100,\n",
              " 'signalling': 101,\n",
              " 'earlier': 102,\n",
              " 'ironclad': 103,\n",
              " 'incredibly': 104,\n",
              " 'faintly': 105,\n",
              " 'well': 106,\n",
              " 'woman': 107,\n",
              " 'cylinder': 108,\n",
              " 'persuaded': 109,\n",
              " 'hands': 110,\n",
              " 'profounder': 111,\n",
              " 'parabolic': 112,\n",
              " 'seems': 113,\n",
              " 'burst': 114,\n",
              " 'scale': 115,\n",
              " 'one': 116,\n",
              " 'ago': 117,\n",
              " 'bit': 118,\n",
              " 'heath': 119,\n",
              " 'who': 120,\n",
              " 'women': 121,\n",
              " 'inhabitants': 122,\n",
              " 'inevitable': 123,\n",
              " 'hole': 124,\n",
              " 'daily': 125,\n",
              " 'intensity': 126,\n",
              " 'hundreds': 127,\n",
              " 'thing': 128,\n",
              " 'room': 129,\n",
              " 'discussed': 130,\n",
              " 'hunting': 131,\n",
              " 'station': 132,\n",
              " 'quarter': 133,\n",
              " 'devices': 134,\n",
              " 'unsuccessful': 135,\n",
              " 'opens': 136,\n",
              " 'gone': 137,\n",
              " 'working': 138,\n",
              " 'torpedo': 139,\n",
              " 'hurrah': 140,\n",
              " 'vestiges': 141,\n",
              " 'polished': 142,\n",
              " 'attempted': 143,\n",
              " 'experience': 144,\n",
              " 'quickly': 145,\n",
              " 'fire': 146,\n",
              " 'did': 147,\n",
              " 'enormous': 148,\n",
              " 'barrow': 149,\n",
              " 'cardboard': 150,\n",
              " 'incrustation': 151,\n",
              " 'moment': 152,\n",
              " 'race': 153,\n",
              " 'joints': 154,\n",
              " 'faded': 155,\n",
              " 'perceive': 156,\n",
              " 'number': 157,\n",
              " 'end': 158,\n",
              " 'everything': 159,\n",
              " 'aliens': 160,\n",
              " 'divided': 161,\n",
              " 'plants': 162,\n",
              " 'state': 163,\n",
              " 'heather': 164,\n",
              " 'such': 165,\n",
              " 'silvery': 166,\n",
              " 'fifty': 167,\n",
              " 'headed': 168,\n",
              " 'fierce': 169,\n",
              " 'humanity': 170,\n",
              " 'watched': 171,\n",
              " 'singing': 172,\n",
              " 'due': 173,\n",
              " 'luminous': 174,\n",
              " 'reading': 175,\n",
              " 'upright': 176,\n",
              " 'called': 177,\n",
              " 'clear': 178,\n",
              " 'alive': 179,\n",
              " 'incandescent': 180,\n",
              " 'screwing': 181,\n",
              " 'matter': 182,\n",
              " 'flames': 183,\n",
              " 'century': 184,\n",
              " 'panting': 185,\n",
              " 'arrival': 186,\n",
              " 'flung': 187,\n",
              " 'harshly': 188,\n",
              " 'awake': 189,\n",
              " 'severe': 190,\n",
              " 'use': 191,\n",
              " 'lad': 192,\n",
              " 'unsuspected': 193,\n",
              " 'spat': 194,\n",
              " 'succumbed': 195,\n",
              " 'cause': 196,\n",
              " 'renewal': 197,\n",
              " 'rounded': 198,\n",
              " 'stated': 199,\n",
              " 'logged': 200,\n",
              " 'active': 201,\n",
              " 'various': 202,\n",
              " 'ni': 203,\n",
              " 'deserted': 204,\n",
              " 'blackness': 205,\n",
              " 'fellow': 206,\n",
              " 'missing': 207,\n",
              " 'structure': 208,\n",
              " 'son': 209,\n",
              " 'bottle': 210,\n",
              " 'were': 211,\n",
              " 'coast': 212,\n",
              " 'hedges': 213,\n",
              " 'largely': 214,\n",
              " 'suggestions': 215,\n",
              " 'looked': 216,\n",
              " 'exchange': 217,\n",
              " 'members': 218,\n",
              " 'what': 219,\n",
              " 'reassure': 220,\n",
              " 'again': 221,\n",
              " 'regardless': 222,\n",
              " 'black': 223,\n",
              " 'return': 224,\n",
              " 'dangers': 225,\n",
              " 'air': 226,\n",
              " 'touch': 227,\n",
              " 'soldiers': 228,\n",
              " 'myself': 229,\n",
              " 'heavy': 230,\n",
              " 'issue': 231,\n",
              " 'caught': 232,\n",
              " 'pressed': 233,\n",
              " 'green': 234,\n",
              " 'remarkable': 235,\n",
              " 'gardening': 236,\n",
              " 'find': 237,\n",
              " 'shouting': 238,\n",
              " 'overtaken': 239,\n",
              " 'roused': 240,\n",
              " 'maybury': 241,\n",
              " 'mass': 242,\n",
              " 'selection': 243,\n",
              " 'major': 244,\n",
              " 'horse': 245,\n",
              " 'persistence': 246,\n",
              " 'captured': 247,\n",
              " 'differences': 248,\n",
              " 'is': 249,\n",
              " 'mortal': 250,\n",
              " 'nature': 251,\n",
              " 'telegraph': 252,\n",
              " 'rose': 253,\n",
              " 'boldly': 254,\n",
              " 'steady': 255,\n",
              " 'caused': 256,\n",
              " 'crack': 257,\n",
              " 'telescope': 258,\n",
              " 'lavelle': 259,\n",
              " 'swept': 260,\n",
              " 'quietly': 261,\n",
              " 'secular': 262,\n",
              " 'received': 263,\n",
              " 'gravitational': 264,\n",
              " 'formed': 265,\n",
              " 'earthward': 266,\n",
              " 'hunched': 267,\n",
              " 'trapped': 268,\n",
              " 'hoofs': 269,\n",
              " 'generally': 270,\n",
              " 'each': 271,\n",
              " 'composition': 272,\n",
              " 'his': 273,\n",
              " 'painfully': 274,\n",
              " 'contained': 275,\n",
              " 'scullery': 276,\n",
              " 'nose': 277,\n",
              " 'equally': 278,\n",
              " 'hit': 279,\n",
              " 'winter': 280,\n",
              " 'vanity': 281,\n",
              " 'listening': 282,\n",
              " 'bystanders': 283,\n",
              " 'scent': 284,\n",
              " 'form': 285,\n",
              " 'lifted': 286,\n",
              " 'finding': 287,\n",
              " 'smoking': 288,\n",
              " 'in': 289,\n",
              " 'ram': 290,\n",
              " 'periodical': 291,\n",
              " 'temperature': 292,\n",
              " 'human': 293,\n",
              " 'man': 294,\n",
              " 'notes': 295,\n",
              " 'impetus': 296,\n",
              " 'forming': 297,\n",
              " 'lonely': 298,\n",
              " 'slowly': 299,\n",
              " 'if': 300,\n",
              " 'out': 301,\n",
              " 'velocity': 302,\n",
              " 'august': 303,\n",
              " 'addressed': 304,\n",
              " 'sudden': 305,\n",
              " 'snatched': 306,\n",
              " 'volcanic': 307,\n",
              " 'boys': 308,\n",
              " 'knowledge': 309,\n",
              " 'twisting': 310,\n",
              " 'present': 311,\n",
              " 'whitened': 312,\n",
              " 'morning': 313,\n",
              " 'hold': 314,\n",
              " 'establish': 315,\n",
              " 'proven': 316,\n",
              " 'care': 317,\n",
              " 'sun': 318,\n",
              " 'military': 319,\n",
              " 'war': 320,\n",
              " 'desisted': 321,\n",
              " 'best': 322,\n",
              " 'anything': 323,\n",
              " 'can': 324,\n",
              " 'apples': 325,\n",
              " 'companion': 326,\n",
              " 'lorded': 327,\n",
              " 'firing': 328,\n",
              " 'bread': 329,\n",
              " 'flame': 330,\n",
              " 'when': 331,\n",
              " 'scrutiny': 332,\n",
              " 'weapon': 333,\n",
              " 'tentacle': 334,\n",
              " 'above': 335,\n",
              " 'impact': 336,\n",
              " 'attack': 337,\n",
              " 'fever': 338,\n",
              " 'coal': 339,\n",
              " 'seemed': 340,\n",
              " 'meteorite': 341,\n",
              " 'hill': 342,\n",
              " 'discipline': 343,\n",
              " 'brought': 344,\n",
              " 'actions': 345,\n",
              " 'spiders': 346,\n",
              " 'multiply': 347,\n",
              " 'mounds': 348,\n",
              " 'plunderers': 349,\n",
              " 'preferred': 350,\n",
              " 'several': 351,\n",
              " 'dead': 352,\n",
              " 'narrative': 353,\n",
              " 'experts': 354,\n",
              " 'officers': 355,\n",
              " 'parallel': 356,\n",
              " 'shopper': 357,\n",
              " 'safety': 358,\n",
              " 'imprisonment': 359,\n",
              " 'sand': 360,\n",
              " 'humans': 361,\n",
              " 'staggered': 362,\n",
              " 'it': 363,\n",
              " 'back': 364,\n",
              " 'anticipate': 365,\n",
              " 'luckily': 366,\n",
              " 'flag': 367,\n",
              " 'three': 368,\n",
              " 'case': 369,\n",
              " 'londonward': 370,\n",
              " 'probably': 371,\n",
              " 'warship': 372,\n",
              " 'angel': 373,\n",
              " 'flood': 374,\n",
              " 'wonder': 375,\n",
              " 'enough': 376,\n",
              " 'choked': 377,\n",
              " 'uncovered': 378,\n",
              " 'action': 379,\n",
              " 'twelfth': 380,\n",
              " 'them': 381,\n",
              " 'trembled': 382,\n",
              " 'glare': 383,\n",
              " 'all': 384,\n",
              " 'under': 385,\n",
              " 'incredible': 386,\n",
              " 'mile': 387,\n",
              " 'oily': 388,\n",
              " 'fallen': 389,\n",
              " 'twin': 390,\n",
              " 'iron': 391,\n",
              " 'remember': 392,\n",
              " 'stood': 393,\n",
              " 'garden': 394,\n",
              " 'around': 395,\n",
              " 'getting': 396,\n",
              " 'thinks': 397,\n",
              " 'adventure': 398,\n",
              " 'authorities': 399,\n",
              " 'resisting': 400,\n",
              " 'think': 401,\n",
              " 'evening': 402,\n",
              " 'bird': 403,\n",
              " '.': 404,\n",
              " 'more': 405,\n",
              " 'unknown': 406,\n",
              " 'mid': 407,\n",
              " 'clearly': 408,\n",
              " 'past': 409,\n",
              " 'eyes': 410,\n",
              " 'concerning': 411,\n",
              " 'dozing': 412,\n",
              " 'big': 413,\n",
              " 'church': 414,\n",
              " 'forced': 415,\n",
              " 'enigma': 416,\n",
              " 'palpitating': 417,\n",
              " 'disease': 418,\n",
              " 'impossibility': 419,\n",
              " 'minds': 420,\n",
              " 'telescopic': 421,\n",
              " 'must': 422,\n",
              " 'dear': 423,\n",
              " 'people': 424,\n",
              " 'aldershot': 425,\n",
              " 'weeping': 426,\n",
              " 'places': 427,\n",
              " 'developments': 428,\n",
              " 'vulgar': 429,\n",
              " 'thirty': 430,\n",
              " 'caked': 431,\n",
              " 'wrought': 432,\n",
              " 'brightness': 433,\n",
              " 'artilleryman': 434,\n",
              " 'until': 435,\n",
              " 'elphinstone': 436,\n",
              " 'indicated': 437,\n",
              " 'afraid': 438,\n",
              " 'faint': 439,\n",
              " 'stimulated': 440,\n",
              " 'drove': 441,\n",
              " 'pits': 442,\n",
              " 'forward': 443,\n",
              " 'rusty': 444,\n",
              " 'majority': 445,\n",
              " 'outbreak': 446,\n",
              " 'wheels': 447,\n",
              " 'others': 448,\n",
              " 'altogether': 449,\n",
              " 'bleached': 450,\n",
              " 'community': 451,\n",
              " 'feet': 452,\n",
              " 'way': 453,\n",
              " 'scattered': 454,\n",
              " 'moved': 455,\n",
              " 'shooting': 456,\n",
              " 'large': 457,\n",
              " 'gravity': 458,\n",
              " 'counties': 459,\n",
              " 'tussle': 460,\n",
              " 'ninty': 461,\n",
              " 'are': 462,\n",
              " 'starlight': 463,\n",
              " 'chopper': 464,\n",
              " 'look': 465,\n",
              " 'staring': 466,\n",
              " 'moral': 467,\n",
              " 'waving': 468,\n",
              " 'bore': 469,\n",
              " 'public': 470,\n",
              " 'time': 471,\n",
              " 'scarcely': 472,\n",
              " 'looks': 473,\n",
              " 'halfway': 474,\n",
              " 'float': 475,\n",
              " 'dangling': 476,\n",
              " 'maxims': 477,\n",
              " 'within': 478,\n",
              " 'calmer': 479,\n",
              " 'terrestrial': 480,\n",
              " 'noisome': 481,\n",
              " 'roses': 482,\n",
              " 'allude': 483,\n",
              " 'commended': 484,\n",
              " 'kind': 485,\n",
              " 'him': 486,\n",
              " 'afternoon': 487,\n",
              " 'deadly': 488,\n",
              " 'minutes': 489,\n",
              " 'howl': 490,\n",
              " 'business': 491,\n",
              " 'wire': 492,\n",
              " 'ear': 493,\n",
              " 'forth': 494,\n",
              " 'choking': 495,\n",
              " 'counted': 496,\n",
              " 'stripes': 497,\n",
              " 'barnet': 498,\n",
              " 'wood': 499,\n",
              " 'star': 500,\n",
              " 'douche': 501,\n",
              " 'hard': 502,\n",
              " 'apparently': 503,\n",
              " 'hand': 504,\n",
              " 'planets': 505,\n",
              " 'special': 506,\n",
              " 'have': 507,\n",
              " 'agreed': 508,\n",
              " 'name': 509,\n",
              " 'coldest': 510,\n",
              " 'light': 511,\n",
              " 'huge': 512,\n",
              " 'incomprehensible': 513,\n",
              " 'leather': 514,\n",
              " 'behaviour': 515,\n",
              " 'laughed': 516,\n",
              " 'fall': 517,\n",
              " 'cleaver': 518,\n",
              " 'crumpled': 519,\n",
              " 'races': 520,\n",
              " 'audible': 521,\n",
              " 'hungry': 522,\n",
              " 'gathered': 523,\n",
              " 's': 524,\n",
              " 'note': 525,\n",
              " 'flashes': 526,\n",
              " 'somewhere': 527,\n",
              " 'gay': 528,\n",
              " 'whispered': 529,\n",
              " 'sounds': 530,\n",
              " 'process': 531,\n",
              " 'riverbank': 532,\n",
              " 'eleven': 533,\n",
              " 'intelligence': 534,\n",
              " 'this': 535,\n",
              " 'like': 536,\n",
              " 'trundling': 537,\n",
              " 'row': 538,\n",
              " 'god': 539,\n",
              " 'set': 540,\n",
              " 'top': 541,\n",
              " 'height': 542,\n",
              " 'sat': 543,\n",
              " 'impinged': 544,\n",
              " 'concerns': 545,\n",
              " 'furtive': 546,\n",
              " 'glowed': 547,\n",
              " 'barracks': 548,\n",
              " 'narrow': 549,\n",
              " 'comfort': 550,\n",
              " 'able': 551,\n",
              " 'astronomical': 552,\n",
              " 'middlesex': 553,\n",
              " 'gride': 554,\n",
              " 'upon': 555,\n",
              " 'raving': 556,\n",
              " 'lunatic': 557,\n",
              " 'ing': 558,\n",
              " 'missiles': 559,\n",
              " 'remembered': 560,\n",
              " 'invisible': 561,\n",
              " 'home': 562,\n",
              " 'drew': 563,\n",
              " 'visible': 564,\n",
              " 'sobered': 565,\n",
              " 'since': 566,\n",
              " 'heaps': 567,\n",
              " 'perpetrated': 568,\n",
              " 'flash': 569,\n",
              " 'shrivelled': 570,\n",
              " 'sequence': 571,\n",
              " 'ground': 572,\n",
              " 'seen': 573,\n",
              " 'java': 574,\n",
              " 'showing': 575,\n",
              " 'startled': 576,\n",
              " 'movements': 577,\n",
              " 'shouted': 578,\n",
              " 'bulged': 579,\n",
              " 'angry': 580,\n",
              " 'passed': 581,\n",
              " 'wires': 582,\n",
              " 'tree': 583,\n",
              " 'wandering': 584,\n",
              " 'become': 585,\n",
              " 'stopped': 586,\n",
              " 'threatening': 587,\n",
              " 'death': 588,\n",
              " 'nest': 589,\n",
              " 'stretched': 590,\n",
              " 'go': 591,\n",
              " 'region': 592,\n",
              " 'needed': 593,\n",
              " 'bat': 594,\n",
              " 'be': 595,\n",
              " 'flaming': 596,\n",
              " 'jet': 597,\n",
              " 'yell': 598,\n",
              " 'means': 599,\n",
              " 'captain': 600,\n",
              " 'felt': 601,\n",
              " 'let': 602,\n",
              " 'heard': 603,\n",
              " 'tomorrow': 604,\n",
              " 'pin': 605,\n",
              " 'seven': 606,\n",
              " 'flashed': 607,\n",
              " 'ten': 608,\n",
              " 'fulham': 609,\n",
              " 'exciting': 610,\n",
              " 'newspaper': 611,\n",
              " 'head': 612,\n",
              " 'then': 613,\n",
              " 'tell': 614,\n",
              " 'multitudes': 615,\n",
              " 'flickered': 616,\n",
              " 'doors': 617,\n",
              " 'rolling': 618,\n",
              " 'eat': 619,\n",
              " 'beyond': 620,\n",
              " 'he': 621,\n",
              " 'rate': 622,\n",
              " 'speaking': 623,\n",
              " 'also': 624,\n",
              " 'glance': 625,\n",
              " 'last': 626,\n",
              " 'would': 627,\n",
              " 'having': 628,\n",
              " 'reception': 629,\n",
              " 'feeble': 630,\n",
              " 'articles': 631,\n",
              " 'vaguely': 632,\n",
              " 'warm': 633,\n",
              " 'cartoon': 634,\n",
              " 'pit': 635,\n",
              " 'wall': 636,\n",
              " 'misadventure': 637,\n",
              " 'five': 638,\n",
              " 't': 639,\n",
              " 'storm': 640,\n",
              " 'met': 641,\n",
              " 'habitants': 642,\n",
              " 'squadron': 643,\n",
              " 'fourteen': 644,\n",
              " 'hearsay': 645,\n",
              " 'aspect': 646,\n",
              " 'martian': 647,\n",
              " 'driver': 648,\n",
              " 'practically': 649,\n",
              " 'meadows': 650,\n",
              " 'lush': 651,\n",
              " 'thunder': 652,\n",
              " 'gulf': 653,\n",
              " 'tenth': 654,\n",
              " 'explosions': 655,\n",
              " 'red': 656,\n",
              " 'yards': 657,\n",
              " 'bury': 658,\n",
              " 'chiefly': 659,\n",
              " 'from': 660,\n",
              " 'coming': 661,\n",
              " 'moving': 662,\n",
              " 'sword': 663,\n",
              " 'pointed': 664,\n",
              " 'perrotin': 665,\n",
              " 'conductivity': 666,\n",
              " 'planet': 667,\n",
              " 'monster': 668,\n",
              " 'for': 669,\n",
              " 'leaping': 670,\n",
              " 'every': 671,\n",
              " 'cry': 672,\n",
              " 'nice': 673,\n",
              " 'four': 674,\n",
              " 'weed': 675,\n",
              " 'absolute': 676,\n",
              " 'branches': 677,\n",
              " 'define': 678,\n",
              " 'came': 679,\n",
              " 'streets': 680,\n",
              " 'eaten': 681,\n",
              " 'began': 682,\n",
              " 'judge': 683,\n",
              " 'astonishingly': 684,\n",
              " 'scratching': 685,\n",
              " 'brittle': 686,\n",
              " 'perplexed': 687,\n",
              " 'softened': 688,\n",
              " 'perception': 689,\n",
              " 'crowd': 690,\n",
              " 'peeking': 691,\n",
              " 'could': 692,\n",
              " 'two': 693,\n",
              " 'drawing': 694,\n",
              " 'succumb': 695,\n",
              " 'civilisation': 696,\n",
              " 'natural': 697,\n",
              " 'comparing': 698,\n",
              " 'blue': 699,\n",
              " 'speculation': 700,\n",
              " 'swarm': 701,\n",
              " 'stark': 702,\n",
              " 'examination': 703,\n",
              " 'recovery': 704,\n",
              " 'direction': 705,\n",
              " 'towards': 706,\n",
              " 'immensely': 707,\n",
              " 'forty': 708,\n",
              " 'during': 709,\n",
              " 'eruption': 710,\n",
              " 'cankering': 711,\n",
              " 'need': 712,\n",
              " 'regiment': 713,\n",
              " 'threatened': 714,\n",
              " 'quantities': 715,\n",
              " 'spoken': 716,\n",
              " 'near': 717,\n",
              " 'choose': 718,\n",
              " 'told': 719,\n",
              " 'with': 720,\n",
              " 'attention': 721,\n",
              " 'troubled': 722,\n",
              " 'wearisome': 723,\n",
              " 'aloo': 724,\n",
              " 'physical': 725,\n",
              " 'low': 726,\n",
              " 'observers': 727,\n",
              " 'unemployed': 728,\n",
              " 'astronomer': 729,\n",
              " 'momentarily': 730,\n",
              " 'perhaps': 731,\n",
              " 'nearer': 732,\n",
              " 'want': 733,\n",
              " 'once': 734,\n",
              " 'beam': 735,\n",
              " 'possibility': 736,\n",
              " 'day': 737,\n",
              " 'marked': 738,\n",
              " 'water': 739,\n",
              " 'streak': 740,\n",
              " 'pyrford': 741,\n",
              " 'sustained': 742,\n",
              " 'our': 743,\n",
              " 'hidden': 744,\n",
              " 'across': 745,\n",
              " 'on': 746,\n",
              " 'journalist': 747,\n",
              " 'news': 748,\n",
              " 'about': 749,\n",
              " 'struggle': 750,\n",
              " 'actual': 751,\n",
              " 'doorway': 752,\n",
              " 'dispached': 753,\n",
              " 'papers': 754,\n",
              " 'face': 755,\n",
              " 'curiosity': 756,\n",
              " 'darkness': 757,\n",
              " 'been': 758,\n",
              " 'away': 759,\n",
              " 'bulk': 760,\n",
              " 'repented': 761,\n",
              " 'district': 762,\n",
              " 'scoffed': 763,\n",
              " 'lid': 764,\n",
              " 'here': 765,\n",
              " 'hussars': 766,\n",
              " 'clock': 767,\n",
              " 'took': 768,\n",
              " 'was': 769,\n",
              " 'answered': 770,\n",
              " 'invaded': 771,\n",
              " 'projectile': 772,\n",
              " 'tumultuous': 773,\n",
              " 'artillery': 774,\n",
              " 'remote': 775,\n",
              " 'henderson': 776,\n",
              " 'meteor': 777,\n",
              " 'generate': 778,\n",
              " 'carrying': 779,\n",
              " 'acquired': 780,\n",
              " 'fourth': 781,\n",
              " 'tried': 782,\n",
              " 'kept': 783,\n",
              " 'violently': 784,\n",
              " 'determination': 785,\n",
              " 'deafening': 786,\n",
              " 'great': 787,\n",
              " 'rations': 788,\n",
              " 'taken': 789,\n",
              " 'leaving': 790,\n",
              " 'nor': 791,\n",
              " 'appearance': 792,\n",
              " 'destruction': 793,\n",
              " 'labours': 794,\n",
              " 'excess': 795,\n",
              " 'denning': 796,\n",
              " 'extermination': 797,\n",
              " 'catching': 798,\n",
              " 'twelve': 799,\n",
              " 'arose': 800,\n",
              " 'blood': 801,\n",
              " 'after': 802,\n",
              " 'didn': 803,\n",
              " 'than': 804,\n",
              " 'meteorites': 805,\n",
              " 'worded': 806,\n",
              " 'law': 807,\n",
              " 'leatherhead': 808,\n",
              " 'really': 809,\n",
              " 'shot': 810,\n",
              " 'bacterial': 811,\n",
              " 'swimming': 812,\n",
              " 'supping': 813,\n",
              " 'an': 814,\n",
              " 'marshall': 815,\n",
              " 'struggling': 816,\n",
              " 'we': 817,\n",
              " 'vain': 818,\n",
              " 'hue': 819,\n",
              " 'grey': 820,\n",
              " 'work': 821,\n",
              " 'laid': 822,\n",
              " 'no': 823,\n",
              " 'over': 824,\n",
              " 'particular': 825,\n",
              " 'foremost': 826,\n",
              " 'demented': 827,\n",
              " 'bodies': 828,\n",
              " 'sorry': 829,\n",
              " 'hour': 830,\n",
              " 'greenish': 831,\n",
              " 'neighbour': 832,\n",
              " 'taking': 833,\n",
              " 'calling': 834,\n",
              " 'slaughter': 835,\n",
              " 'world': 836,\n",
              " 'another': 837,\n",
              " 'wife': 838,\n",
              " 'bacteria': 839,\n",
              " 'happy': 840,\n",
              " 'invited': 841,\n",
              " 'common': 842,\n",
              " 'heat': 843,\n",
              " 'civilised': 844,\n",
              " 'wine': 845,\n",
              " 'field': 846,\n",
              " 'establishing': 847,\n",
              " 'before': 848,\n",
              " 'space': 849,\n",
              " 'into': 850,\n",
              " 'had': 851,\n",
              " 'couple': 852,\n",
              " 'editions': 853,\n",
              " 'cooling': 854,\n",
              " 'so': 855,\n",
              " 'street': 856,\n",
              " 'sky': 857,\n",
              " 'examined': 858,\n",
              " 'between': 859,\n",
              " 'fasten': 860,\n",
              " 'however': 861,\n",
              " 'secure': 862,\n",
              " 'landward': 863,\n",
              " 'fascination': 864,\n",
              " 'amusing': 865,\n",
              " 'because': 866,\n",
              " 'against': 867,\n",
              " 'hanging': 868,\n",
              " 'questions': 869,\n",
              " 'window': 870,\n",
              " 'times': 871,\n",
              " 'own': 872,\n",
              " 'amount': 873,\n",
              " 'concentrate': 874,\n",
              " 'non': 875,\n",
              " 'while': 876,\n",
              " 'years': 877,\n",
              " 'power': 878,\n",
              " 'full': 879,\n",
              " 'upperworks': 880,\n",
              " 'i': 881,\n",
              " 'got': 882,\n",
              " 'shipful': 883,\n",
              " 'there': 884,\n",
              " 'younger': 885,\n",
              " 'himself': 886,\n",
              " 'throwing': 887,\n",
              " 'food': 888,\n",
              " 'steadily': 889,\n",
              " 'inferior': 890,\n",
              " 'brevity': 891,\n",
              " 'attracting': 892,\n",
              " 'approaches': 893,\n",
              " 'animals': 894,\n",
              " 'ogilvy': 895,\n",
              " 'coloured': 896,\n",
              " 'mystery': 897,\n",
              " 'species': 898,\n",
              " 'tottering': 899,\n",
              " 'left': 900,\n",
              " 'dozed': 901,\n",
              " 'already': 902,\n",
              " 'drinking': 903,\n",
              " 'scaly': 904,\n",
              " 'animal': 905,\n",
              " 'repeated': 906,\n",
              " 'mutton': 907,\n",
              " 'small': 908,\n",
              " 'rather': 909,\n",
              " 'dawn': 910,\n",
              " 'gleam': 911,\n",
              " 'cage': 912,\n",
              " 'surface': 913,\n",
              " 'spoke': 914,\n",
              " 'john': 915,\n",
              " 'curious': 916,\n",
              " 'towers': 917,\n",
              " 'carmine': 918,\n",
              " 'curate': 919,\n",
              " 'hundred': 920,\n",
              " 'reported': 921,\n",
              " 'quite': 922,\n",
              " 'sight': 923,\n",
              " 'glistened': 924,\n",
              " 'hydrogen': 925,\n",
              " 'by': 926,\n",
              " 'miles': 927,\n",
              " 'cried': 928,\n",
              " 'struck': 929,\n",
              " 'runs': 930,\n",
              " 'creatures': 931,\n",
              " 'souls': 932,\n",
              " 'astonishing': 933,\n",
              " 'earthly': 934,\n",
              " 'greyish': 935,\n",
              " 'ladies': 936,\n",
              " 'transitory': 937,\n",
              " 'latter': 938,\n",
              " 'suburbs': 939,\n",
              " 'mound': 940,\n",
              " 'knee': 941,\n",
              " 'cellar': 942,\n",
              " 'of': 943,\n",
              " 'haggard': 944,\n",
              " 'mirror': 945,\n",
              " 'knew': 946,\n",
              " 'sheet': 947,\n",
              " 'bicycle': 948,\n",
              " 'heart': 949,\n",
              " 'greatly': 950,\n",
              " 'authority': 951,\n",
              " 'theory': 952,\n",
              " 'sailors': 953,\n",
              " 'yet': 954,\n",
              " 'cross': 955,\n",
              " 'someday': 956,\n",
              " 'projects': 957,\n",
              " 'yellowish': 958,\n",
              " 'few': 959,\n",
              " 'ordinary': 960,\n",
              " 'spade': 961,\n",
              " 'said': 962,\n",
              " 'leading': 963,\n",
              " 'spread': 964,\n",
              " 'wisdom': 965,\n",
              " 'thought': 966,\n",
              " 'brokenly': 967,\n",
              " 'northern': 968,\n",
              " 'fitful': 969,\n",
              " 'come': 970,\n",
              " 'nights': 971,\n",
              " 'removed': 972,\n",
              " 'imminent': 973,\n",
              " 'she': 974,\n",
              " 'effort': 975,\n",
              " 'bison': 976,\n",
              " 'size': 977,\n",
              " 'pace': 978,\n",
              " 'political': 979,\n",
              " 'country': 980,\n",
              " 'rising': 981,\n",
              " 'dozen': 982,\n",
              " 'realise': 983,\n",
              " 'machines': 984,\n",
              " 'discussing': 985,\n",
              " 'fusillade': 986,\n",
              " 'those': 987,\n",
              " 'developed': 988,\n",
              " 'pony': 989,\n",
              " 'chanced': 990,\n",
              " 'themselves': 991,\n",
              " '1894': 992,\n",
              " 'wanted': 993,\n",
              " 'put': 994,\n",
              " 'rushing': 995,\n",
              " 'st': 996,\n",
              " 'side': 997,\n",
              " 'ignorance': 998,\n",
              " 'noise': 999,\n",
              " 'lit': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ckqMeojvKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating the story, question and answer from the training set\n",
        "\n",
        "train_story_text = []\n",
        "train_question_text = []\n",
        "train_answer_text = []\n",
        "\n",
        "for story,question,answer in train_data:\n",
        "  train_story_text.append(story)\n",
        "  train_question_text.append(question)\n",
        "  train_answer_text.append(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzEwzz-elrQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_story_text[:1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2sLmQokls_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the story , question and answer in numerical form\n",
        "\n",
        "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G_xGpMPnG0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_story_seq[:1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0aM8MQinIjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n",
        " \n",
        "  # X = STORIES\n",
        "  X = []\n",
        "  # Xq = QUERY/QUESTION\n",
        "  Xq = []\n",
        "  # Y = CORRECT ANSWER\n",
        "  Y = []\n",
        "  \n",
        "  \n",
        "  for story, query, answer in data:\n",
        "      \n",
        "    x = [word_index[word.lower()] for word in story]\n",
        "    xq = [word_index[word.lower()] for word in query]\n",
        "\n",
        "    # Index 0 is reserved so we're going to use + 1\n",
        "    y = np.zeros(len(word_index) + 1)\n",
        "    \n",
        "    \n",
        "    y[word_index[answer]] = 1\n",
        "    \n",
        "    X.append(x)\n",
        "    Xq.append(xq)\n",
        "    Y.append(y)\n",
        "      \n",
        "  # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n",
        "      \n",
        "  return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ORx454yH58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_train, queries_train, answers_train = vectorize_stories(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Qbq2hSyLEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs_test, queries_test, answers_test = vectorize_stories(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcke-ta2yNjD",
        "colab_type": "code",
        "outputId": "3da79313-204e-4f57-e9bd-229795fbf58e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "inputs_train"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,  943,  739,  404],\n",
              "       [   0,    0,    0, ...,  555,  381,  404],\n",
              "       [   0,    0,    0, ...,  804,  743, 1054],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  886, 1389,  404],\n",
              "       [   0,    0,    0, ...,  289,  923,  404],\n",
              "       [   0,    0,    0, ...,  360,  442,  404]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMc_KeZcyWEh",
        "colab_type": "code",
        "outputId": "08797d5f-0ce0-4ccf-e2d0-41ada57010e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "inputs_test"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ...,   97, 1296,  404],\n",
              "       [   0,    0,    0, ...,  769,  189,  404],\n",
              "       [   0,    0,    0, ..., 1252,  343,  404],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  886, 1389,  404],\n",
              "       [   0,    0,    0, ...,  289,  923,  404],\n",
              "       [   0,    0,    0, ...,  360,  442,  404]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou85hftJyZTT",
        "colab_type": "code",
        "outputId": "2cbabd0f-a986-4392-a78a-67ea68a22a20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "answers_test"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvdg3AGzK5zu",
        "colab_type": "text"
      },
      "source": [
        "## 5) Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1RM4RWRycqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
        "from keras.layers import add, dot, concatenate\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAsGotd4K_tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We have two inputs, stories and questions. So we need to use placeholders. Input() is used to instantiate a Keras tensor.\n",
        "# PlaceHolder shape = (max_story_len,batch_size)\n",
        "\n",
        "input_sequence = Input((max_story_len,))\n",
        "question = Input((max_question_len,))\n",
        "\n",
        "vocab_size = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JTOViNpOmiC",
        "colab_type": "text"
      },
      "source": [
        "Input Encoder m"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnHQXaXMDBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input gets embedded to a sequence of vectors\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\n",
        "input_encoder_m.add(Dropout(0.3))\n",
        "\n",
        "# This encoder will output:\n",
        "# (samples, story_maxlen, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeJAU9LDOq8T",
        "colab_type": "text"
      },
      "source": [
        "Input Encoder c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaGsssBROQHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embed the input into a sequence of vectors of size query_maxlen\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\n",
        "input_encoder_c.add(Dropout(0.3))\n",
        "# output: (samples, story_maxlen, query_maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U5o1QlwOzZe",
        "colab_type": "text"
      },
      "source": [
        "Question Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE-zF4Y1OcTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embed the question into a sequence of vectors\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size,output_dim=64,input_length=max_question_len))\n",
        "question_encoder.add(Dropout(0.3))\n",
        "# output: (samples, query_maxlen, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVzYWX-pRHlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode input sequence and questions (which are indices)\n",
        "# to sequences of dense vectors\n",
        "\n",
        "# ENCODED <---- ENCODER (INPUT)\n",
        "\n",
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRbw6Z8qRJYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape: `(samples, story_maxlen, query_maxlen)`\n",
        "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxadf04bS3xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add the match matrix with the second input vector sequence\n",
        "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
        "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkPmWuGhS6iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the match matrix with the question vector sequence\n",
        "answer = concatenate([response, question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfzstpYBS9Vy",
        "colab_type": "code",
        "outputId": "3c1d245d-23b8-4707-e079-894f12f459bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answer"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'concatenate_1/concat:0' shape=(None, 14, 183) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mjdm2ZlaTQht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce with RNN (LSTM)\n",
        "answer = LSTM(32)(answer)  # (samples, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsADqRvFTUx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Regularization with Dropout\n",
        "answer = Dropout(0.5)(answer)\n",
        "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seq0720uTYLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we output a probability distribution over the vocabulary\n",
        "answer = Activation('softmax')(answer)\n",
        "\n",
        "# build the final model\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGLZFRWDTpu_",
        "colab_type": "code",
        "outputId": "1af7e118-1665-4f84-82cf-9add376ebc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 119)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 14)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             95168       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 14, 64)       95168       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 119, 14)      0           sequential_1[1][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 119, 14)      0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       multiple             20818       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 119, 14)      0           activation_1[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 14, 119)      0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 14, 183)      0           permute_1[0][0]                  \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 32)           27648       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1487)         49071       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 1487)         0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 287,873\n",
            "Trainable params: 287,873\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8tZzpKIXBkh",
        "colab_type": "text"
      },
      "source": [
        "## 6) Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9ZgrHLNTvwS",
        "colab_type": "code",
        "outputId": "2bd00869-2cf3-44f4-ab2d-bf0effe69418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train\n",
        "history = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=150)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "112/112 [==============================] - 1s 5ms/step - loss: 7.2441 - accuracy: 0.1607\n",
            "Epoch 2/150\n",
            "112/112 [==============================] - 0s 750us/step - loss: 6.9470 - accuracy: 0.5179\n",
            "Epoch 3/150\n",
            "112/112 [==============================] - 0s 750us/step - loss: 6.3741 - accuracy: 0.5536\n",
            "Epoch 4/150\n",
            "112/112 [==============================] - 0s 792us/step - loss: 5.8956 - accuracy: 0.5446\n",
            "Epoch 5/150\n",
            "112/112 [==============================] - 0s 794us/step - loss: 5.4635 - accuracy: 0.5446\n",
            "Epoch 6/150\n",
            "112/112 [==============================] - 0s 770us/step - loss: 5.0416 - accuracy: 0.5804\n",
            "Epoch 7/150\n",
            "112/112 [==============================] - 0s 758us/step - loss: 4.7108 - accuracy: 0.5536\n",
            "Epoch 8/150\n",
            "112/112 [==============================] - 0s 735us/step - loss: 4.3424 - accuracy: 0.5893\n",
            "Epoch 9/150\n",
            "112/112 [==============================] - 0s 793us/step - loss: 4.1595 - accuracy: 0.5893\n",
            "Epoch 10/150\n",
            "112/112 [==============================] - 0s 788us/step - loss: 3.7705 - accuracy: 0.6071\n",
            "Epoch 11/150\n",
            "112/112 [==============================] - 0s 736us/step - loss: 3.4863 - accuracy: 0.5357\n",
            "Epoch 12/150\n",
            "112/112 [==============================] - 0s 699us/step - loss: 3.2568 - accuracy: 0.5089\n",
            "Epoch 13/150\n",
            "112/112 [==============================] - 0s 701us/step - loss: 2.8575 - accuracy: 0.5714\n",
            "Epoch 14/150\n",
            "112/112 [==============================] - 0s 735us/step - loss: 2.6242 - accuracy: 0.5804\n",
            "Epoch 15/150\n",
            "112/112 [==============================] - 0s 696us/step - loss: 2.4945 - accuracy: 0.5268\n",
            "Epoch 16/150\n",
            "112/112 [==============================] - 0s 716us/step - loss: 2.2781 - accuracy: 0.5536\n",
            "Epoch 17/150\n",
            "112/112 [==============================] - 0s 729us/step - loss: 2.1159 - accuracy: 0.6429\n",
            "Epoch 18/150\n",
            "112/112 [==============================] - 0s 739us/step - loss: 1.8198 - accuracy: 0.6161\n",
            "Epoch 19/150\n",
            "112/112 [==============================] - 0s 856us/step - loss: 1.9848 - accuracy: 0.4643\n",
            "Epoch 20/150\n",
            "112/112 [==============================] - 0s 739us/step - loss: 1.7653 - accuracy: 0.5089\n",
            "Epoch 21/150\n",
            "112/112 [==============================] - 0s 840us/step - loss: 1.5325 - accuracy: 0.6071\n",
            "Epoch 22/150\n",
            "112/112 [==============================] - 0s 727us/step - loss: 1.5680 - accuracy: 0.5714\n",
            "Epoch 23/150\n",
            "112/112 [==============================] - 0s 749us/step - loss: 1.3863 - accuracy: 0.5536\n",
            "Epoch 24/150\n",
            "112/112 [==============================] - 0s 744us/step - loss: 1.3369 - accuracy: 0.5714\n",
            "Epoch 25/150\n",
            "112/112 [==============================] - 0s 716us/step - loss: 1.2965 - accuracy: 0.5893\n",
            "Epoch 26/150\n",
            "112/112 [==============================] - 0s 706us/step - loss: 1.1653 - accuracy: 0.6161\n",
            "Epoch 27/150\n",
            "112/112 [==============================] - 0s 720us/step - loss: 1.1010 - accuracy: 0.5982\n",
            "Epoch 28/150\n",
            "112/112 [==============================] - 0s 713us/step - loss: 1.0894 - accuracy: 0.5982\n",
            "Epoch 29/150\n",
            "112/112 [==============================] - 0s 818us/step - loss: 1.0041 - accuracy: 0.5536\n",
            "Epoch 30/150\n",
            "112/112 [==============================] - 0s 735us/step - loss: 1.0431 - accuracy: 0.5625\n",
            "Epoch 31/150\n",
            "112/112 [==============================] - 0s 742us/step - loss: 1.0892 - accuracy: 0.5179\n",
            "Epoch 32/150\n",
            "112/112 [==============================] - 0s 726us/step - loss: 0.9414 - accuracy: 0.5893\n",
            "Epoch 33/150\n",
            "112/112 [==============================] - 0s 817us/step - loss: 0.8999 - accuracy: 0.5446\n",
            "Epoch 34/150\n",
            "112/112 [==============================] - 0s 798us/step - loss: 0.8993 - accuracy: 0.5893\n",
            "Epoch 35/150\n",
            "112/112 [==============================] - 0s 743us/step - loss: 0.8415 - accuracy: 0.5357\n",
            "Epoch 36/150\n",
            "112/112 [==============================] - 0s 721us/step - loss: 0.8509 - accuracy: 0.6429\n",
            "Epoch 37/150\n",
            "112/112 [==============================] - 0s 711us/step - loss: 0.8494 - accuracy: 0.5625\n",
            "Epoch 38/150\n",
            "112/112 [==============================] - 0s 702us/step - loss: 0.8279 - accuracy: 0.5804\n",
            "Epoch 39/150\n",
            "112/112 [==============================] - 0s 782us/step - loss: 0.8263 - accuracy: 0.6250\n",
            "Epoch 40/150\n",
            "112/112 [==============================] - 0s 739us/step - loss: 0.8456 - accuracy: 0.5893\n",
            "Epoch 41/150\n",
            "112/112 [==============================] - 0s 774us/step - loss: 0.8400 - accuracy: 0.5625\n",
            "Epoch 42/150\n",
            "112/112 [==============================] - 0s 734us/step - loss: 0.7989 - accuracy: 0.5804\n",
            "Epoch 43/150\n",
            "112/112 [==============================] - 0s 727us/step - loss: 0.7869 - accuracy: 0.5536\n",
            "Epoch 44/150\n",
            "112/112 [==============================] - 0s 733us/step - loss: 0.7495 - accuracy: 0.5625\n",
            "Epoch 45/150\n",
            "112/112 [==============================] - 0s 768us/step - loss: 0.7694 - accuracy: 0.5536\n",
            "Epoch 46/150\n",
            "112/112 [==============================] - 0s 697us/step - loss: 0.7838 - accuracy: 0.5893\n",
            "Epoch 47/150\n",
            "112/112 [==============================] - 0s 706us/step - loss: 0.7596 - accuracy: 0.5536\n",
            "Epoch 48/150\n",
            "112/112 [==============================] - 0s 728us/step - loss: 0.7260 - accuracy: 0.5804\n",
            "Epoch 49/150\n",
            "112/112 [==============================] - 0s 714us/step - loss: 0.7550 - accuracy: 0.5804\n",
            "Epoch 50/150\n",
            "112/112 [==============================] - 0s 709us/step - loss: 0.7196 - accuracy: 0.6071\n",
            "Epoch 51/150\n",
            "112/112 [==============================] - 0s 699us/step - loss: 0.7461 - accuracy: 0.6071\n",
            "Epoch 52/150\n",
            "112/112 [==============================] - 0s 704us/step - loss: 0.7302 - accuracy: 0.5804\n",
            "Epoch 53/150\n",
            "112/112 [==============================] - 0s 732us/step - loss: 0.7427 - accuracy: 0.5714\n",
            "Epoch 54/150\n",
            "112/112 [==============================] - 0s 737us/step - loss: 0.7592 - accuracy: 0.5893\n",
            "Epoch 55/150\n",
            "112/112 [==============================] - 0s 707us/step - loss: 0.7553 - accuracy: 0.5446\n",
            "Epoch 56/150\n",
            "112/112 [==============================] - 0s 715us/step - loss: 0.7371 - accuracy: 0.5982\n",
            "Epoch 57/150\n",
            "112/112 [==============================] - 0s 781us/step - loss: 0.7157 - accuracy: 0.5804\n",
            "Epoch 58/150\n",
            "112/112 [==============================] - 0s 728us/step - loss: 0.7012 - accuracy: 0.5804\n",
            "Epoch 59/150\n",
            "112/112 [==============================] - 0s 727us/step - loss: 0.6989 - accuracy: 0.5982\n",
            "Epoch 60/150\n",
            "112/112 [==============================] - 0s 706us/step - loss: 0.7110 - accuracy: 0.5446\n",
            "Epoch 61/150\n",
            "112/112 [==============================] - 0s 707us/step - loss: 0.7197 - accuracy: 0.5714\n",
            "Epoch 62/150\n",
            "112/112 [==============================] - 0s 700us/step - loss: 0.7139 - accuracy: 0.5982\n",
            "Epoch 63/150\n",
            "112/112 [==============================] - 0s 711us/step - loss: 0.7059 - accuracy: 0.5625\n",
            "Epoch 64/150\n",
            "112/112 [==============================] - 0s 950us/step - loss: 0.7132 - accuracy: 0.5804\n",
            "Epoch 65/150\n",
            "112/112 [==============================] - 0s 765us/step - loss: 0.7199 - accuracy: 0.5714\n",
            "Epoch 66/150\n",
            "112/112 [==============================] - 0s 749us/step - loss: 0.7046 - accuracy: 0.5893\n",
            "Epoch 67/150\n",
            "112/112 [==============================] - 0s 780us/step - loss: 0.7005 - accuracy: 0.5982\n",
            "Epoch 68/150\n",
            "112/112 [==============================] - 0s 717us/step - loss: 0.7110 - accuracy: 0.5893\n",
            "Epoch 69/150\n",
            "112/112 [==============================] - 0s 818us/step - loss: 0.6900 - accuracy: 0.6071\n",
            "Epoch 70/150\n",
            "112/112 [==============================] - 0s 753us/step - loss: 0.6944 - accuracy: 0.5893\n",
            "Epoch 71/150\n",
            "112/112 [==============================] - 0s 722us/step - loss: 0.6867 - accuracy: 0.5893\n",
            "Epoch 72/150\n",
            "112/112 [==============================] - 0s 720us/step - loss: 0.7106 - accuracy: 0.5982\n",
            "Epoch 73/150\n",
            "112/112 [==============================] - 0s 694us/step - loss: 0.6947 - accuracy: 0.5804\n",
            "Epoch 74/150\n",
            "112/112 [==============================] - 0s 698us/step - loss: 0.7001 - accuracy: 0.5893\n",
            "Epoch 75/150\n",
            "112/112 [==============================] - 0s 702us/step - loss: 0.6940 - accuracy: 0.5982\n",
            "Epoch 76/150\n",
            "112/112 [==============================] - 0s 699us/step - loss: 0.6885 - accuracy: 0.5893\n",
            "Epoch 77/150\n",
            "112/112 [==============================] - 0s 705us/step - loss: 0.6824 - accuracy: 0.6071\n",
            "Epoch 78/150\n",
            "112/112 [==============================] - 0s 758us/step - loss: 0.6878 - accuracy: 0.5982\n",
            "Epoch 79/150\n",
            "112/112 [==============================] - 0s 715us/step - loss: 0.6964 - accuracy: 0.5804\n",
            "Epoch 80/150\n",
            "112/112 [==============================] - 0s 774us/step - loss: 0.6973 - accuracy: 0.5982\n",
            "Epoch 81/150\n",
            "112/112 [==============================] - 0s 828us/step - loss: 0.6824 - accuracy: 0.5982\n",
            "Epoch 82/150\n",
            "112/112 [==============================] - 0s 827us/step - loss: 0.6899 - accuracy: 0.5893\n",
            "Epoch 83/150\n",
            "112/112 [==============================] - 0s 801us/step - loss: 0.6792 - accuracy: 0.6071\n",
            "Epoch 84/150\n",
            "112/112 [==============================] - 0s 778us/step - loss: 0.6591 - accuracy: 0.6071\n",
            "Epoch 85/150\n",
            "112/112 [==============================] - 0s 788us/step - loss: 0.7042 - accuracy: 0.5893\n",
            "Epoch 86/150\n",
            "112/112 [==============================] - 0s 783us/step - loss: 0.6897 - accuracy: 0.6071\n",
            "Epoch 87/150\n",
            "112/112 [==============================] - 0s 913us/step - loss: 0.6856 - accuracy: 0.5982\n",
            "Epoch 88/150\n",
            "112/112 [==============================] - 0s 719us/step - loss: 0.6787 - accuracy: 0.6071\n",
            "Epoch 89/150\n",
            "112/112 [==============================] - 0s 730us/step - loss: 0.6917 - accuracy: 0.5893\n",
            "Epoch 90/150\n",
            "112/112 [==============================] - 0s 719us/step - loss: 0.6855 - accuracy: 0.5982\n",
            "Epoch 91/150\n",
            "112/112 [==============================] - 0s 748us/step - loss: 0.6798 - accuracy: 0.5893\n",
            "Epoch 92/150\n",
            "112/112 [==============================] - 0s 774us/step - loss: 0.6774 - accuracy: 0.5893\n",
            "Epoch 93/150\n",
            "112/112 [==============================] - 0s 785us/step - loss: 0.6764 - accuracy: 0.5982\n",
            "Epoch 94/150\n",
            "112/112 [==============================] - 0s 966us/step - loss: 0.6794 - accuracy: 0.5982\n",
            "Epoch 95/150\n",
            "112/112 [==============================] - 0s 717us/step - loss: 0.6865 - accuracy: 0.6071\n",
            "Epoch 96/150\n",
            "112/112 [==============================] - 0s 779us/step - loss: 0.6767 - accuracy: 0.5982\n",
            "Epoch 97/150\n",
            "112/112 [==============================] - 0s 724us/step - loss: 0.6816 - accuracy: 0.5982\n",
            "Epoch 98/150\n",
            "112/112 [==============================] - 0s 708us/step - loss: 0.6894 - accuracy: 0.5893\n",
            "Epoch 99/150\n",
            "112/112 [==============================] - 0s 698us/step - loss: 0.6849 - accuracy: 0.5982\n",
            "Epoch 100/150\n",
            "112/112 [==============================] - 0s 709us/step - loss: 0.6750 - accuracy: 0.5982\n",
            "Epoch 101/150\n",
            "112/112 [==============================] - 0s 784us/step - loss: 0.6728 - accuracy: 0.5982\n",
            "Epoch 102/150\n",
            "112/112 [==============================] - 0s 715us/step - loss: 0.6706 - accuracy: 0.5982\n",
            "Epoch 103/150\n",
            "112/112 [==============================] - 0s 705us/step - loss: 0.6792 - accuracy: 0.5982\n",
            "Epoch 104/150\n",
            "112/112 [==============================] - 0s 698us/step - loss: 0.6735 - accuracy: 0.5982\n",
            "Epoch 105/150\n",
            "112/112 [==============================] - 0s 765us/step - loss: 0.6842 - accuracy: 0.5982\n",
            "Epoch 106/150\n",
            "112/112 [==============================] - 0s 821us/step - loss: 0.6793 - accuracy: 0.5893\n",
            "Epoch 107/150\n",
            "112/112 [==============================] - 0s 729us/step - loss: 0.6910 - accuracy: 0.5893\n",
            "Epoch 108/150\n",
            "112/112 [==============================] - 0s 736us/step - loss: 0.6740 - accuracy: 0.5982\n",
            "Epoch 109/150\n",
            "112/112 [==============================] - 0s 735us/step - loss: 0.6871 - accuracy: 0.5982\n",
            "Epoch 110/150\n",
            "112/112 [==============================] - 0s 718us/step - loss: 0.6788 - accuracy: 0.5982\n",
            "Epoch 111/150\n",
            "112/112 [==============================] - 0s 719us/step - loss: 0.6836 - accuracy: 0.5893\n",
            "Epoch 112/150\n",
            "112/112 [==============================] - 0s 715us/step - loss: 0.6782 - accuracy: 0.5982\n",
            "Epoch 113/150\n",
            "112/112 [==============================] - 0s 707us/step - loss: 0.6647 - accuracy: 0.5982\n",
            "Epoch 114/150\n",
            "112/112 [==============================] - 0s 686us/step - loss: 0.6783 - accuracy: 0.5982\n",
            "Epoch 115/150\n",
            "112/112 [==============================] - 0s 700us/step - loss: 0.6703 - accuracy: 0.6071\n",
            "Epoch 116/150\n",
            "112/112 [==============================] - 0s 782us/step - loss: 0.6902 - accuracy: 0.5804\n",
            "Epoch 117/150\n",
            "112/112 [==============================] - 0s 768us/step - loss: 0.6764 - accuracy: 0.5982\n",
            "Epoch 118/150\n",
            "112/112 [==============================] - 0s 737us/step - loss: 0.6677 - accuracy: 0.5982\n",
            "Epoch 119/150\n",
            "112/112 [==============================] - 0s 718us/step - loss: 0.6802 - accuracy: 0.5893\n",
            "Epoch 120/150\n",
            "112/112 [==============================] - 0s 710us/step - loss: 0.6768 - accuracy: 0.6161\n",
            "Epoch 121/150\n",
            "112/112 [==============================] - 0s 712us/step - loss: 0.6660 - accuracy: 0.5982\n",
            "Epoch 122/150\n",
            "112/112 [==============================] - 0s 707us/step - loss: 0.6730 - accuracy: 0.5982\n",
            "Epoch 123/150\n",
            "112/112 [==============================] - 0s 829us/step - loss: 0.6514 - accuracy: 0.5982\n",
            "Epoch 124/150\n",
            "112/112 [==============================] - 0s 718us/step - loss: 0.6880 - accuracy: 0.5982\n",
            "Epoch 125/150\n",
            "112/112 [==============================] - 0s 734us/step - loss: 0.6737 - accuracy: 0.5893\n",
            "Epoch 126/150\n",
            "112/112 [==============================] - 0s 797us/step - loss: 0.6626 - accuracy: 0.6071\n",
            "Epoch 127/150\n",
            "112/112 [==============================] - 0s 714us/step - loss: 0.6568 - accuracy: 0.5982\n",
            "Epoch 128/150\n",
            "112/112 [==============================] - 0s 749us/step - loss: 0.6705 - accuracy: 0.6161\n",
            "Epoch 129/150\n",
            "112/112 [==============================] - 0s 775us/step - loss: 0.6546 - accuracy: 0.5982\n",
            "Epoch 130/150\n",
            "112/112 [==============================] - 0s 711us/step - loss: 0.6618 - accuracy: 0.5982\n",
            "Epoch 131/150\n",
            "112/112 [==============================] - 0s 714us/step - loss: 0.6606 - accuracy: 0.5982\n",
            "Epoch 132/150\n",
            "112/112 [==============================] - 0s 727us/step - loss: 0.6639 - accuracy: 0.5982\n",
            "Epoch 133/150\n",
            "112/112 [==============================] - 0s 737us/step - loss: 0.6472 - accuracy: 0.6071\n",
            "Epoch 134/150\n",
            "112/112 [==============================] - 0s 720us/step - loss: 0.6464 - accuracy: 0.5982\n",
            "Epoch 135/150\n",
            "112/112 [==============================] - 0s 757us/step - loss: 0.6340 - accuracy: 0.6071\n",
            "Epoch 136/150\n",
            "112/112 [==============================] - 0s 720us/step - loss: 0.6290 - accuracy: 0.6071\n",
            "Epoch 137/150\n",
            "112/112 [==============================] - 0s 699us/step - loss: 0.6194 - accuracy: 0.5982\n",
            "Epoch 138/150\n",
            "112/112 [==============================] - 0s 764us/step - loss: 0.6103 - accuracy: 0.6250\n",
            "Epoch 139/150\n",
            "112/112 [==============================] - 0s 747us/step - loss: 0.6094 - accuracy: 0.6518\n",
            "Epoch 140/150\n",
            "112/112 [==============================] - 0s 783us/step - loss: 0.5995 - accuracy: 0.6696\n",
            "Epoch 141/150\n",
            "112/112 [==============================] - 0s 868us/step - loss: 0.5705 - accuracy: 0.6786\n",
            "Epoch 142/150\n",
            "112/112 [==============================] - 0s 802us/step - loss: 0.5571 - accuracy: 0.7232\n",
            "Epoch 143/150\n",
            "112/112 [==============================] - 0s 791us/step - loss: 0.5593 - accuracy: 0.7500\n",
            "Epoch 144/150\n",
            "112/112 [==============================] - 0s 800us/step - loss: 0.5207 - accuracy: 0.7946\n",
            "Epoch 145/150\n",
            "112/112 [==============================] - 0s 822us/step - loss: 0.5176 - accuracy: 0.7679\n",
            "Epoch 146/150\n",
            "112/112 [==============================] - 0s 742us/step - loss: 0.4851 - accuracy: 0.8125\n",
            "Epoch 147/150\n",
            "112/112 [==============================] - 0s 747us/step - loss: 0.4853 - accuracy: 0.8482\n",
            "Epoch 148/150\n",
            "112/112 [==============================] - 0s 710us/step - loss: 0.4929 - accuracy: 0.8571\n",
            "Epoch 149/150\n",
            "112/112 [==============================] - 0s 783us/step - loss: 0.4802 - accuracy: 0.8750\n",
            "Epoch 150/150\n",
            "112/112 [==============================] - 0s 758us/step - loss: 0.4165 - accuracy: 0.9196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1COdF1W_Wi",
        "colab_type": "text"
      },
      "source": [
        "## 7) Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHOIWFdhUuAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelname = 'QA_model.h5'\n",
        "model.save(modelname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRdSBciCXY7B",
        "colab_type": "text"
      },
      "source": [
        "## 8) Plotting the graph of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQJqqK0GXQx4",
        "colab_type": "code",
        "outputId": "c5f2f74b-4309-4b49-bd15-acd639bb6b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkNoTYt_X9oK",
        "colab_type": "text"
      },
      "source": [
        "## 9) Prediction on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOz-aXe1XiTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(modelname)\n",
        "pred_results = model.predict(([inputs_test, queries_test]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A21gS5jSZMBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68a86b94-8312-463e-e313-a9cb70eb04c5"
      },
      "source": [
        "len(pred_results)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVUHZZIcYMeu",
        "colab_type": "code",
        "outputId": "ad3772b8-b062-45ac-e011-85aebb27bc41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "' '.join(test_data[0][0])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i put out my hand and felt the meat chopper hanging to the wall . in a flash i was after him . i was fierce with fear . before he was halfway across the kitchen i had overtaken him . with one last touch of humanity i turned the blade back and struck the curate with the meat chopper . curate went headlong forward and lay stretched on the ground . i stumbled over curate and stood panting . he lay still .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-e_pa-AYP1g",
        "colab_type": "code",
        "outputId": "7ffcaa52-7122-4796-c0ae-8f97f01709c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data[0][1]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['did', 'curate', 'went', 'headlong', 'forward', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpfWL2FpYYyS",
        "colab_type": "code",
        "outputId": "f064b381-b1ed-42ad-fb85-1f3899e64460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data[0][2]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-xR1jY-YpVD",
        "colab_type": "code",
        "outputId": "ea65f8be-a119-4e1e-c337-b20af5693860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred_results[0]   # Probability of occurence of each word from vocab in the answer."
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.33856361e-07, 1.04094944e-07, 9.58991322e-08, ...,\n",
              "       1.23092661e-07, 1.13178956e-07, 1.32650385e-07], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_whrg2YjoS",
        "colab_type": "code",
        "outputId": "40d8891a-1a78-4b33-c6e7-ae674cf388d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Generating prediction from model\n",
        "\n",
        "val_max = np.argmax(pred_results[0])\n",
        "val_max"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro5MXlYBYbPU",
        "colab_type": "code",
        "outputId": "c44322cb-1205-4717-da44-15796dbd3ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "for key, val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "\n",
        "print(\"Predicted answer is: \", k)\n",
        "print(\"Probability of certainty was: \", pred_results[0][val_max])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted answer is:  yes\n",
            "Probability of certainty was:  0.6539801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFn0Lv2yZqba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c28029a-5cde-44d6-a81d-b49a22adf199"
      },
      "source": [
        "for res in pred_results:\n",
        "  print(res)\n",
        "  break"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.33856361e-07 1.04094944e-07 9.58991322e-08 ... 1.23092661e-07\n",
            " 1.13178956e-07 1.32650385e-07]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYFdvAXodffI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual_results = ['yes','yes','no','yes','no','yes','no','yes','no','no','yes','yes',]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nfqM9UBZPhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking for all the test data set\n",
        "ans = []\n",
        "for result in pred_results:\n",
        "\n",
        "  val_max = np.argmax(result)\n",
        "\n",
        "  for key, val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "\n",
        "  ans.append(k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VH9r9-Hdxra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd9c003a-10d5-4569-e1a1-97ad1cb8bd16"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(ans,actual_results)*100\n",
        "accuracy = float(\"{:.2f}\".format(accuracy))\n",
        "print(f'Accuracy: {accuracy}%')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 91.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUDELjLx4YP2",
        "colab_type": "text"
      },
      "source": [
        "# <CENTER> THE END"
      ]
    }
  ]
}