{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Script Generator.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QAs8LxFeB72p"
      },
      "source": [
        "#Predicting a sequence of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgXORaHQDyNU",
        "colab_type": "code",
        "outputId": "9612f1ba-85b1-4534-8a28-9d929f0c29ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wv1uvScN-et_"
      },
      "source": [
        "## 1) Importing the basic libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TdN5S4phBpdN",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-PfbmjOm-nbn"
      },
      "source": [
        "## 2) Reading the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HarfUZOCW78",
        "outputId": "838c320f-ba76-4a16-8f7f-680c0a0d8e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('drive/My Drive/the_war_of_the_worlds.txt') as f:\n",
        "  text = f.read()\n",
        "\n",
        "len(text), type(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(336832, str)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQ-M-GZMDSme",
        "outputId": "79ebbb8c-45db-494e-e392-a62c37ab2d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BOOK ONE\\n\\nTHE COMING OF THE MARTIANS\\n\\nCHAPTER ONE \\n \\nTHE EVE OF THE WAR\\n \\nNo one would have believed'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RH7ATppeIsMu",
        "outputId": "11aab28d-71fe-437b-b134-783edecd26da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK ONE\n",
            "\n",
            "THE COMING OF THE MARTIANS\n",
            "\n",
            "CHAPTER ONE \n",
            " \n",
            "THE EVE OF THE WAR\n",
            " \n",
            "No one would have believed in the last years of the nineteenth century that this world was being watched keenly and closely by intelligences greater than man’s and yet as mortal as his own; that as men busied themselves about their various concerns they were scrutinised and studied, perhaps almost as narrowly as a man with a microscope might scrutinise the transient creatures that swarm and multiply in a drop of water. Wit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V7lhn_R0JEe3"
      },
      "source": [
        "## 3) Encoding and Decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s8bmKO3FI4tw",
        "outputId": "60080c00-e2a7-44fe-8f23-ac19ce2909f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# We are going for character level encoding\n",
        "\n",
        "character_set = set(text)\n",
        "print(f'{len(character_set)} \\n')\n",
        "print(np.array(character_set))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80 \n",
            "\n",
            "{'G', 'X', '\\n', 'I', 'z', '2', 'Q', 'u', '’', 'J', 't', 'V', '.', 'C', 'T', '0', 'x', '‘', 'y', 'v', '\\u2003', '(', '3', 'o', 'O', 'R', 'r', 'Y', '5', '1', '-', 'j', 'A', 'm', '?', '8', 'k', 'f', '9', 'Z', 'H', 'd', '4', 'L', '!', '…', 'M', 'E', 's', 'p', 'q', 'l', ',', ';', 'S', 'i', 'B', 'w', 'D', ')', 'U', 'a', '\\t', 'n', 'e', '—', 'F', 'K', 'b', 'N', 'W', '\"', ':', '&', '#', 'g', 'P', 'h', ' ', 'c'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufVa508yJUb6",
        "colab": {}
      },
      "source": [
        "#Creating encoder and decoder\n",
        "\n",
        "#Decoder (num --> letter)\n",
        "\n",
        "decoder = dict(enumerate(character_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X9xCpDdOKDkd",
        "colab": {}
      },
      "source": [
        "#Encoder (letter --> num)\n",
        "\n",
        "encoder = {each:idx for idx,each in decoder.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AfbVw_wgKme2",
        "outputId": "77272396-fdfc-462c-95c1-280ed2903a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Converting the text into numerical values\n",
        "\n",
        "encoded_text = np.array([encoder[ch] for ch in text])\n",
        "encoded_text[:500]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56, 24, 24, 67, 78, 24, 69, 47,  2,  2, 14, 40, 47, 78, 13, 24, 46,\n",
              "        3, 69,  0, 78, 24, 66, 78, 14, 40, 47, 78, 46, 32, 25, 14,  3, 32,\n",
              "       69, 54,  2,  2, 13, 40, 32, 76, 14, 47, 25, 78, 24, 69, 47, 78,  2,\n",
              "       78,  2, 14, 40, 47, 78, 47, 11, 47, 78, 24, 66, 78, 14, 40, 47, 78,\n",
              "       70, 32, 25,  2, 78,  2, 69, 23, 78, 23, 63, 64, 78, 57, 23,  7, 51,\n",
              "       41, 78, 77, 61, 19, 64, 78, 68, 64, 51, 55, 64, 19, 64, 41, 78, 55,\n",
              "       63, 78, 10, 77, 64, 78, 51, 61, 48, 10, 78, 18, 64, 61, 26, 48, 78,\n",
              "       23, 37, 78, 10, 77, 64, 78, 63, 55, 63, 64, 10, 64, 64, 63, 10, 77,\n",
              "       78, 79, 64, 63, 10,  7, 26, 18, 78, 10, 77, 61, 10, 78, 10, 77, 55,\n",
              "       48, 78, 57, 23, 26, 51, 41, 78, 57, 61, 48, 78, 68, 64, 55, 63, 75,\n",
              "       78, 57, 61, 10, 79, 77, 64, 41, 78, 36, 64, 64, 63, 51, 18, 78, 61,\n",
              "       63, 41, 78, 79, 51, 23, 48, 64, 51, 18, 78, 68, 18, 78, 55, 63, 10,\n",
              "       64, 51, 51, 55, 75, 64, 63, 79, 64, 48, 78, 75, 26, 64, 61, 10, 64,\n",
              "       26, 78, 10, 77, 61, 63, 78, 33, 61, 63,  8, 48, 78, 61, 63, 41, 78,\n",
              "       18, 64, 10, 78, 61, 48, 78, 33, 23, 26, 10, 61, 51, 78, 61, 48, 78,\n",
              "       77, 55, 48, 78, 23, 57, 63, 53, 78, 10, 77, 61, 10, 78, 61, 48, 78,\n",
              "       33, 64, 63, 78, 68,  7, 48, 55, 64, 41, 78, 10, 77, 64, 33, 48, 64,\n",
              "       51, 19, 64, 48, 78, 61, 68, 23,  7, 10, 78, 10, 77, 64, 55, 26, 78,\n",
              "       19, 61, 26, 55, 23,  7, 48, 78, 79, 23, 63, 79, 64, 26, 63, 48, 78,\n",
              "       10, 77, 64, 18, 78, 57, 64, 26, 64, 78, 48, 79, 26,  7, 10, 55, 63,\n",
              "       55, 48, 64, 41, 78, 61, 63, 41, 78, 48, 10,  7, 41, 55, 64, 41, 52,\n",
              "       78, 49, 64, 26, 77, 61, 49, 48, 78, 61, 51, 33, 23, 48, 10, 78, 61,\n",
              "       48, 78, 63, 61, 26, 26, 23, 57, 51, 18, 78, 61, 48, 78, 61, 78, 33,\n",
              "       61, 63, 78, 57, 55, 10, 77, 78, 61, 78, 33, 55, 79, 26, 23, 48, 79,\n",
              "       23, 49, 64, 78, 33, 55, 75, 77, 10, 78, 48, 79, 26,  7, 10, 55, 63,\n",
              "       55, 48, 64, 78, 10, 77, 64, 78, 10, 26, 61, 63, 48, 55, 64, 63, 10,\n",
              "       78, 79, 26, 64, 61, 10,  7, 26, 64, 48, 78, 10, 77, 61, 10, 78, 48,\n",
              "       57, 61, 26, 33, 78, 61, 63, 41, 78, 33,  7, 51, 10, 55, 49, 51, 18,\n",
              "       78, 55, 63, 78, 61, 78, 41, 26, 23, 49, 78, 23, 37, 78, 57, 61, 10,\n",
              "       64, 26, 12, 78, 70, 55, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHpDC8_E8zFP",
        "outputId": "65a13f4d-165d-4e77-a9c8-2ca66deead40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoded_text.flatten()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([56, 24, 24, ..., 78, 78,  2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BQOlDxSs9jqX"
      },
      "source": [
        "To convert a numpy array into one hot encoding : https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tzdQlnQX-wao"
      },
      "source": [
        "### 3.1) Creating one-hot encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YeCafhB63Ox4",
        "colab": {}
      },
      "source": [
        "#Creating one-hot encoder for the encoded_text\n",
        "\n",
        "def one_hot_encoder(encoded_text,num_uni_chars):\n",
        "  #encoded text ---> Batch of encoded text\n",
        "  #num_uni_chars ---> len(set(text))\n",
        "\n",
        "  one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
        "\n",
        "  one_hot = one_hot.astype(np.float32)  #for tensor\n",
        "\n",
        "  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
        "\n",
        "  one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4QEJRaT58T-",
        "outputId": "33ebe8c1-84f4-4e59-bd98-e9b54d3d0552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "a = np.array([1,2,0])\n",
        "print(a)\n",
        "one_hot_encoder(a,3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "glL2X_kt-1Va"
      },
      "source": [
        "## 4) Generating Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jtehZVh5-JPR",
        "colab": {}
      },
      "source": [
        "def generate_batches(encoded_text,samp_per_batch=10,seq_length=50):\n",
        "  \n",
        "  # X  = encoded text of len sequence length(x to x+50)  \n",
        "  # y = encoded text of len sequence length(x+1 to x+51)\n",
        "  # For example : if encoded_text has [0,1,2,3,4,5,6] and seq_len = 3, then X = [0,1,2] , y = [1,2,3]\n",
        "\n",
        "  #How many characters per batch ??\n",
        "  char_per_batch = samp_per_batch * seq_length\n",
        "\n",
        "  # How many batches we can make, given the length of encoding text ??\n",
        "  num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "\n",
        "  #Cut off the borders of the encoded text that does not fit in this range\n",
        "  encoded_text = encoded_text[:char_per_batch * num_batches_avail]\n",
        "\n",
        "  encoded_text = encoded_text.reshape((samp_per_batch,-1))\n",
        "\n",
        "  # Go through each row in array.\n",
        "  for n in range(0, encoded_text.shape[1], seq_length):\n",
        "      \n",
        "    # Grab feature characters\n",
        "    x = encoded_text[:, n:n+seq_length]\n",
        "    \n",
        "    # y is the target shifted over by 1\n",
        "    y = np.zeros_like(x)\n",
        "    \n",
        "    #\n",
        "    try:\n",
        "        y[:, :-1] = x[:, 1:]\n",
        "        y[:, -1]  = encoded_text[:, n+seq_length]\n",
        "        \n",
        "    # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
        "    except:\n",
        "        y[:, :-1] = x[:, 1:]\n",
        "        y[:, -1] = encoded_text[:, 0]\n",
        "        \n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MY4ZCWkjaz8Z",
        "outputId": "564b2c47-cda9-4eca-dd3e-8bddd70e0933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_text = np.arange(20)\n",
        "sample_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EVlD7_R8a8uu",
        "outputId": "656767c4-1cb0-45a8-8f02-02e17a7acd94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sam = generate_batches(sample_text,2,10)\n",
        "x,y = next(sam)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2  3  4  5  6  7  8  9]\n",
            " [10 11 12 13 14 15 16 17 18 19]]\n",
            "[[ 1  2  3  4  5  6  7  8  9  0]\n",
            " [11 12 13 14 15 16 17 18 19 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GYVJR9RNdF1K"
      },
      "source": [
        "## 5) Creating the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkrkrbRcbd2n",
        "colab": {}
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "    \n",
        "  def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "      \n",
        "      \n",
        "      # SET UP ATTRIBUTES\n",
        "      super().__init__()\n",
        "      self.drop_prob = drop_prob\n",
        "      self.num_layers = num_layers\n",
        "      self.num_hidden = num_hidden\n",
        "      self.use_gpu = use_gpu\n",
        "      \n",
        "      #CHARACTER SET, ENCODER, and DECODER\n",
        "      self.all_chars = all_chars\n",
        "      self.decoder = dict(enumerate(all_chars))\n",
        "      self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "      \n",
        "      \n",
        "      self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "      \n",
        "      self.dropout = nn.Dropout(drop_prob)\n",
        "      \n",
        "      self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "    \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "                \n",
        "      \n",
        "      lstm_output, hidden = self.lstm(x, hidden)\n",
        "      \n",
        "      \n",
        "      drop_output = self.dropout(lstm_output)\n",
        "      \n",
        "      drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "      \n",
        "      \n",
        "      final_out = self.fc_linear(drop_output)\n",
        "      \n",
        "      \n",
        "      return final_out, hidden\n",
        "  \n",
        "  \n",
        "  def hidden_state(self, batch_size):\n",
        "      \n",
        "      if self.use_gpu:\n",
        "          \n",
        "          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                    torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "      else:\n",
        "          hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                    torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "      \n",
        "      return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cr9lDn4VicCG",
        "colab": {}
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=character_set,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TkFmfbrlilxy",
        "colab": {}
      },
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJ3bGemBjGlm",
        "outputId": "ad04cfb6-8537-44e9-964a-9df2c3bad928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(total_param)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5460048"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cu1rGAJojRGo"
      },
      "source": [
        "## 6) Optimization and Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JTsPBScjKJW",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I9r61KwUkK_I"
      },
      "source": [
        "## 7) Training Data and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VuM9iZMcjWA-",
        "outputId": "dcb84b40-fe8f-4ad8-ff89-df75cf00993d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_percent = 0.9\n",
        "int(len(encoded_text)*train_percent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "303148"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZFiE-_JkbRM",
        "colab": {}
      },
      "source": [
        "train_ind = int(len(encoded_text)*train_percent)\n",
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Cpz3wbwk4mu"
      },
      "source": [
        "## 8) Training the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRTjTVMqk1P2",
        "colab": {}
      },
      "source": [
        "## VARIABLES\n",
        "\n",
        "# Epochs to train for\n",
        "epochs = 100\n",
        "# batch size \n",
        "batch_size = 128\n",
        "\n",
        "# Length of sequence\n",
        "seq_len = 100\n",
        "\n",
        "# for printing report purposes\n",
        "# always start at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_text)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "08m-fuxllIdo",
        "outputId": "1fb5e127-3620-4edf-b38e-61b7c5343563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "  hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "  for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "      \n",
        "    tracker += 1\n",
        "    \n",
        "    x = one_hot_encoder(x,num_char)\n",
        "    inputs = torch.from_numpy(x)\n",
        "    targets = torch.from_numpy(y)\n",
        "    if model.use_gpu:\n",
        "        \n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        \n",
        "    # Reset Hidden State\n",
        "    # If we dont' reset we would backpropagate through all training history\n",
        "    hidden = tuple([state.data for state in hidden])\n",
        "    \n",
        "    model.zero_grad()\n",
        "    \n",
        "    lstm_output, hidden = model.forward(inputs,hidden)\n",
        "    loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "    # LET\"S CLIP JUST IN CASE\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    ### CHECK ON VALIDATION SET ######\n",
        "    \n",
        "    if tracker % 25 == 0:\n",
        "       \n",
        "      val_hidden = model.hidden_state(batch_size)\n",
        "      val_losses = []\n",
        "      model.eval()\n",
        "      \n",
        "      for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "  \n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "\n",
        "        if model.use_gpu:\n",
        "\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "        val_hidden = tuple([state.data for state in val_hidden])\n",
        "        \n",
        "        lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "        val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "      \n",
        "      # Reset to training model after val for loop\n",
        "      model.train()\n",
        "      \n",
        "      print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Step: 25 Val Loss: 3.022536516189575\n",
            "Epoch: 2 Step: 50 Val Loss: 3.0179052352905273\n",
            "Epoch: 3 Step: 75 Val Loss: 3.01595401763916\n",
            "Epoch: 4 Step: 100 Val Loss: 3.004434823989868\n",
            "Epoch: 5 Step: 125 Val Loss: 2.8606786727905273\n",
            "Epoch: 6 Step: 150 Val Loss: 2.695924758911133\n",
            "Epoch: 7 Step: 175 Val Loss: 2.626570463180542\n",
            "Epoch: 8 Step: 200 Val Loss: 2.526420831680298\n",
            "Epoch: 9 Step: 225 Val Loss: 2.362206220626831\n",
            "Epoch: 10 Step: 250 Val Loss: 2.2275004386901855\n",
            "Epoch: 11 Step: 275 Val Loss: 2.156747341156006\n",
            "Epoch: 13 Step: 300 Val Loss: 2.094712495803833\n",
            "Epoch: 14 Step: 325 Val Loss: 2.041759967803955\n",
            "Epoch: 15 Step: 350 Val Loss: 1.9899237155914307\n",
            "Epoch: 16 Step: 375 Val Loss: 1.9497240781784058\n",
            "Epoch: 17 Step: 400 Val Loss: 1.9127224683761597\n",
            "Epoch: 18 Step: 425 Val Loss: 1.8756123781204224\n",
            "Epoch: 19 Step: 450 Val Loss: 1.8424971103668213\n",
            "Epoch: 20 Step: 475 Val Loss: 1.8128924369812012\n",
            "Epoch: 21 Step: 500 Val Loss: 1.7901405096054077\n",
            "Epoch: 22 Step: 525 Val Loss: 1.7627147436141968\n",
            "Epoch: 23 Step: 550 Val Loss: 1.7344413995742798\n",
            "Epoch: 24 Step: 575 Val Loss: 1.7095446586608887\n",
            "Epoch: 26 Step: 600 Val Loss: 1.6884260177612305\n",
            "Epoch: 27 Step: 625 Val Loss: 1.6678721904754639\n",
            "Epoch: 28 Step: 650 Val Loss: 1.6436342000961304\n",
            "Epoch: 29 Step: 675 Val Loss: 1.6254059076309204\n",
            "Epoch: 30 Step: 700 Val Loss: 1.6085312366485596\n",
            "Epoch: 31 Step: 725 Val Loss: 1.591854453086853\n",
            "Epoch: 32 Step: 750 Val Loss: 1.5818196535110474\n",
            "Epoch: 33 Step: 775 Val Loss: 1.5642350912094116\n",
            "Epoch: 34 Step: 800 Val Loss: 1.5593055486679077\n",
            "Epoch: 35 Step: 825 Val Loss: 1.5512996912002563\n",
            "Epoch: 36 Step: 850 Val Loss: 1.5303019285202026\n",
            "Epoch: 38 Step: 875 Val Loss: 1.5223164558410645\n",
            "Epoch: 39 Step: 900 Val Loss: 1.5140278339385986\n",
            "Epoch: 40 Step: 925 Val Loss: 1.506462812423706\n",
            "Epoch: 41 Step: 950 Val Loss: 1.4964079856872559\n",
            "Epoch: 42 Step: 975 Val Loss: 1.4845705032348633\n",
            "Epoch: 43 Step: 1000 Val Loss: 1.4824408292770386\n",
            "Epoch: 44 Step: 1025 Val Loss: 1.4736746549606323\n",
            "Epoch: 45 Step: 1050 Val Loss: 1.4756464958190918\n",
            "Epoch: 46 Step: 1075 Val Loss: 1.4683917760849\n",
            "Epoch: 47 Step: 1100 Val Loss: 1.4674937725067139\n",
            "Epoch: 48 Step: 1125 Val Loss: 1.4641313552856445\n",
            "Epoch: 49 Step: 1150 Val Loss: 1.4506345987319946\n",
            "Epoch: 51 Step: 1175 Val Loss: 1.449754238128662\n",
            "Epoch: 52 Step: 1200 Val Loss: 1.4515904188156128\n",
            "Epoch: 53 Step: 1225 Val Loss: 1.4515838623046875\n",
            "Epoch: 54 Step: 1250 Val Loss: 1.4487172365188599\n",
            "Epoch: 55 Step: 1275 Val Loss: 1.4460655450820923\n",
            "Epoch: 56 Step: 1300 Val Loss: 1.4487159252166748\n",
            "Epoch: 57 Step: 1325 Val Loss: 1.4487130641937256\n",
            "Epoch: 58 Step: 1350 Val Loss: 1.4492207765579224\n",
            "Epoch: 59 Step: 1375 Val Loss: 1.4440439939498901\n",
            "Epoch: 60 Step: 1400 Val Loss: 1.4488447904586792\n",
            "Epoch: 61 Step: 1425 Val Loss: 1.4501540660858154\n",
            "Epoch: 63 Step: 1450 Val Loss: 1.4496334791183472\n",
            "Epoch: 64 Step: 1475 Val Loss: 1.4500586986541748\n",
            "Epoch: 65 Step: 1500 Val Loss: 1.4500350952148438\n",
            "Epoch: 66 Step: 1525 Val Loss: 1.461849570274353\n",
            "Epoch: 67 Step: 1550 Val Loss: 1.4595049619674683\n",
            "Epoch: 68 Step: 1575 Val Loss: 1.4587616920471191\n",
            "Epoch: 69 Step: 1600 Val Loss: 1.4612399339675903\n",
            "Epoch: 70 Step: 1625 Val Loss: 1.469192385673523\n",
            "Epoch: 71 Step: 1650 Val Loss: 1.4637391567230225\n",
            "Epoch: 72 Step: 1675 Val Loss: 1.4648174047470093\n",
            "Epoch: 73 Step: 1700 Val Loss: 1.4605364799499512\n",
            "Epoch: 74 Step: 1725 Val Loss: 1.4671783447265625\n",
            "Epoch: 76 Step: 1750 Val Loss: 1.4807875156402588\n",
            "Epoch: 77 Step: 1775 Val Loss: 1.476300835609436\n",
            "Epoch: 78 Step: 1800 Val Loss: 1.4807051420211792\n",
            "Epoch: 79 Step: 1825 Val Loss: 1.4863184690475464\n",
            "Epoch: 80 Step: 1850 Val Loss: 1.4962055683135986\n",
            "Epoch: 81 Step: 1875 Val Loss: 1.4957213401794434\n",
            "Epoch: 82 Step: 1900 Val Loss: 1.4984130859375\n",
            "Epoch: 83 Step: 1925 Val Loss: 1.4972376823425293\n",
            "Epoch: 84 Step: 1950 Val Loss: 1.5134990215301514\n",
            "Epoch: 85 Step: 1975 Val Loss: 1.5207539796829224\n",
            "Epoch: 86 Step: 2000 Val Loss: 1.514894723892212\n",
            "Epoch: 88 Step: 2025 Val Loss: 1.5169411897659302\n",
            "Epoch: 89 Step: 2050 Val Loss: 1.521888256072998\n",
            "Epoch: 90 Step: 2075 Val Loss: 1.5237245559692383\n",
            "Epoch: 91 Step: 2100 Val Loss: 1.5328129529953003\n",
            "Epoch: 92 Step: 2125 Val Loss: 1.5352773666381836\n",
            "Epoch: 93 Step: 2150 Val Loss: 1.5315029621124268\n",
            "Epoch: 94 Step: 2175 Val Loss: 1.539480447769165\n",
            "Epoch: 95 Step: 2200 Val Loss: 1.5558549165725708\n",
            "Epoch: 96 Step: 2225 Val Loss: 1.5665643215179443\n",
            "Epoch: 97 Step: 2250 Val Loss: 1.5667589902877808\n",
            "Epoch: 98 Step: 2275 Val Loss: 1.5670440196990967\n",
            "Epoch: 99 Step: 2300 Val Loss: 1.5686655044555664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zBTT-LlQ5J0S"
      },
      "source": [
        "## 9) Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9G8MjUOOoefH",
        "colab": {}
      },
      "source": [
        "model_name = 'war_of_worlds_trained_model.net'\n",
        "torch.save(model.state_dict(),model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z2_IdNhR5aO7"
      },
      "source": [
        "## 10) Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W4gE9jfm5VLj",
        "colab": {}
      },
      "source": [
        "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
        "\n",
        "model = CharModel(\n",
        "    all_chars=character_set,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-lMimGsM5sNb",
        "outputId": "f36feb68-9406-4341-dbc3-f9bdee4a5dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(80, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o1-yqiZz52rP"
      },
      "source": [
        "## 11) Generating the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DRZlofwM5zna",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "  # Encode raw letters with model\n",
        "  encoded_text = model.encoder[char]\n",
        "  \n",
        "  # set as numpy array for one hot encoding\n",
        "  # NOTE THE [[ ]] dimensions!!\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  \n",
        "  # One hot encoding\n",
        "  encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "  \n",
        "  # Convert to Tensor\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "  \n",
        "  # Check for CPU\n",
        "  if(model.use_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  \n",
        "  \n",
        "  # Grab hidden states\n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "  \n",
        "  \n",
        "  # Run model and get predicted output\n",
        "  lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "  \n",
        "  # Convert lstm_out to probabilities\n",
        "  probs = F.softmax(lstm_out, dim=1).data\n",
        "   \n",
        "  if(model.use_gpu):\n",
        "    # move back to CPU to use with numpy\n",
        "    probs = probs.cpu()\n",
        "\n",
        "  \n",
        "  # k determines how many characters to consider\n",
        "  # for our probability choice.\n",
        "  # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "  \n",
        "  # Return k largest probabilities in tensor\n",
        "  probs, index_positions = probs.topk(k)\n",
        "  \n",
        "  \n",
        "  index_positions = index_positions.numpy().squeeze()\n",
        "  \n",
        "  # Create array of probabilities\n",
        "  probs = probs.numpy().flatten()\n",
        "  \n",
        "  # Convert to probabilities per index\n",
        "  probs = probs/probs.sum()\n",
        "  \n",
        "  # randomly choose a character based on probabilities\n",
        "  char = np.random.choice(index_positions, p=probs)\n",
        "  \n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return model.decoder[char], hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9aoM0Rs8cas",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "      \n",
        "  # CHECK FOR GPU\n",
        "  if(model.use_gpu):\n",
        "    model.cuda()\n",
        "  else:\n",
        "    model.cpu()\n",
        "  \n",
        "  # Evaluation mode\n",
        "  model.eval()\n",
        "  \n",
        "  # begin output from initial seed\n",
        "  output_chars = [c for c in seed]\n",
        "  \n",
        "  # intiate hidden state\n",
        "  hidden = model.hidden_state(1)\n",
        "  \n",
        "  # predict the next character for every character in seed\n",
        "  for char in seed:\n",
        "    char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "  \n",
        "  # add initial characters to output\n",
        "  output_chars.append(char)\n",
        "  \n",
        "  # Now generate for size requested\n",
        "  for i in range(size):\n",
        "          \n",
        "    # predict based off very last letter in output_chars\n",
        "    char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "    \n",
        "    # add predicted character\n",
        "    output_chars.append(char)\n",
        "  \n",
        "  # return string of predicted text\n",
        "  return ''.join(output_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z1ji7tus82jS",
        "outputId": "99bfe62a-b1e9-4771-b3fb-38127b163de9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(generate_text(model, 1500, seed='The ', k=3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sun screwing off the place. \n",
            "The sudden shopper was a shift flashes of staring at the steamer, and a huge sheet of half away the church the country across the law and face into the road, but a silent country and things about the flood in the sand pits. \n",
            "It was a flowing of gunners and terrible. The fact that his changed as my brother struck the common, so that they had been bright against the heat of the sun came to the common. \n",
            "At the top of Weybridge were staring and saw the flashes and staring at the streets, something off and raining from house from his store, but in a strange and small and dost of fire interment that at last into a share and cloud of all the crackled and clamper, as where I was not for the former that had been bruesed their shouts of the pit. I should have seemed to have been a sort of staring from that dirty black distance at last, and the flashes of smoke or flight, threating and strange and strange and smashed and smulted in tries, and so forth, a strange and smoke or bory of every shift of stream over the sand food, and stood in the steamer, and when the street of my flash as the cread of black smoke as it completely and the strees and struck three from Mars and deserted struck and broken through the chaired and clouded into the road through the road, but as it had some terrestrial strange and sturbly but not a brought up, and the trees again in the crachled and staring up at its treely against the sun startled into the real, and driven to and fro the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hhl_5tgq9C6_",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}